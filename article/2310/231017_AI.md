# 글만 알던 생성AI…영상 보고 감정 읽는 '멀티모달'로
## 챗GPT 출시 1년 만에
## 기술 트렌드 확 바뀌어

## 언어모델, 텍스트 위주로 학습
## 멀티모달은 사진·음성도 인식
## X레이 보고 골절 부위 찾아내
## 자율주행차 등 활용 무궁무진

## 'GPT-4V' 공개한 오픈AI 선두
## 구글 이르면 내달 '제미니' 출시
[글만 알던 생성AI…영상 보고 감정 읽는 '멀티모달'로 - 한국경제](https://n.news.naver.com/article/newspaper/015/0004902756?date=20231017)

---

`대규모멀티모달모델(LMM)이 글로벌 인공지능(AI)산업의 게임 체인저로 주목받고 있다.` **생성형 AI 챗GPT 열풍으로 대규모언어모델(LLM) 개발 경쟁이 본격화한 지 1년 만에 기술 트렌드에 변화가 생긴 것이다.**

멀티모달은 텍스트, 이미지, 음성, 영상 등의 데이터로 훈련해 다양한 결과물을 내놓을 수 있는 모델이다. 최근 등장한 LMM은 더욱 복잡한 이미지 분석과 추론 능력을 갖췄다. `AI업계에선 LMM이 LLM을 대신하는 용어로 자리 잡을 것이라는 전망이 나온다.` 오픈AI와 구글 등이 LMM 개발에 속도를 내고 있어서다.

#### “표정 보고 감정 알아내”

15일(현지시간) AI업계에 따르면 오픈AI는 최근 보고서를 통해 이달 초 공개한 LMM ‘GPT-4V’의 최신 기능을 분석했다. V는 비전(vision)의 첫 글자다. `가장 큰 특징은 텍스트와 이미지 사이의 경계가 지워진 것이다. 이전의 GPT-3, 4에서 텍스트 프롬프트(명령어)를 사용하는 것처럼 쉽게 이미지로 상호 작용할 수 있다는 것이다.`

예를 들어 김밥 만드는 사진을 무질서하게 제시한 뒤 순서대로 정렬하라고 명령하면 정확하게 숙제를 해결했다. 맥주와 물병 등이 놓인 테이블 사진과 메뉴판을 함께 제시하면 식당에 얼마를 내야 하는지도 계산했다. 엑스레이와 컴퓨터단층촬영(CT) 사진을 보고 골절 부위 등 질병을 진단할 수 있었고, 다양한 표정의 얼굴 사진을 보여주면 화남, 놀람, 실망 등의 감정을 묘사했다. 사진뿐만 아니라 그래프, 도형, 표, 사진 속에 등장한 언어까지도 인식해 분석했다. **GPT-4V는 20개 언어를 감지할 수 있다고 보고서는 소개했다.**

멀티모달 개념은 기존에도 있었다. 다만 텍스트보다 인식 과정이 복잡하고, 방대한 데이터로 훈련해야 해 완성도 높은 멀티모달모델이 나오기까진 상당한 시간이 필요할 것으로 예상됐다. `개발자들은 LLM을 기본으로 여기에 소프트웨어를 붙여 이미지 등의 정보를 이해하는 기존 구조를 버렸다. 대신 이미지 등 다양한 형태의 정보를 이해할 수 있는 LMM 코어를 새로 개발했다.` 소프트웨어와 LLM을 병용할 때보다 인식률, 속도 등에서 상당한 개선이 이뤄질 수 있었던 배경이다. `‘멀티모달 LLM’이란 용어도 기존 모델과 구분하기 위해 ‘LMM’으로 대체했다.`

#### “인간-컴퓨터 상호작용 고도화”

`전문가들은 오픈AI의 GPT-4V가 LMM 부문에서 가장 앞섰다고 보고 있다` 텍스트 중심의 챗GPT에 이어 다음 단계에서도 오픈AI가 유리한 입지를 선점한 것이다. 다른 빅테크도 앞다퉈 멀티모달 기술 고도화에 나서고 있다. `메타가 지난달 말 ‘메타 커넥트 2023’에서 내놓은 ‘메타 AI’도 이미지 인식 및 생성 등 멀티모달 기능을 갖췄다.`

`마이크로소프트 연구진이 미국 위스콘신-매디슨대·컬럼비아대 연구진과 함께 개발한 ‘라바 1.5’도 상당히 우수한 LMM이라는 평가를 받는다.` 라바 1.5는 깃허브에 오픈소스로 최근 공개됐다. `구글도 이르면 다음달 ‘제미니’를 공개할 예정이다. 구글의 방대한 유튜브 콘텐츠로 학습한 멀티모달 기능을 구현할 것으로 보인다.` 업계 관계자는 “현재 멀티모달 AI는 이미지와 영상 생성 기능이 인식 능력에 비해 떨어진다”며 “이 문제를 빠르게 해결하는 기업이 LMM 시대에 리더십을 가져갈 수 있을 것”이라고 말했다.

생성형 AI의 멀티모달 기능이 고도화할수록 결과물에 대한 사실 여부를 가려내기 힘들 것이라는 지적도 나온다. 이미 AI 기술을 활용한 딥페이크, 영상 및 음성 조작이 현실화했다. 이스라엘과 하마스의 무력 충돌 이후 가짜뉴스가 각종 소셜미디어에 범람한 것도 이런 기술에 대한 활용도가 높아진 영향이란 분석이다.

---

### 용어정리

* 멀티모달
    * 사람과 기계 사이의 커뮤니케이션은 전통적으로 키보드를 통해 텍스트가 주로 사용되어 옴
    * 스마트폰의 등장 이후 단말기가 소형화되면서 키보드를 이용한 사람과 기계 사이의 커뮤니케이션은 한계에 달함
    * 이에, 음성, 제스처, 펜 등을 활용한 멀티 모달 인터페이스가 발전
    * 여기에 AI 기술의 발달로 음성처리 기술이 획기적으로 발전하면서, 사람과 기계 사이의 커뮤니케이션은 의사소통까지도 가능한 사용자 친화형 기술로 변모하는 중
    * 즉, 멀티 모달은 여러 가지 형태와 의미로 컴퓨터와 대화하는 환경을 뜻함
        * 모달(=모달리티, modality) : 인터랙션 과정에서 사용되는 의사소통 채널
        * 예를 들어, 우리가 PC에 무언가를 입력하려고 할 때 사용하는 키보드나 마우스 등이 하나의 모달리티를 위한 디바이스라고 할 수 있음
        * 모달리티란 한 가지 방식의 사람-컴퓨터 의사소통 ‘채널’을 뜻함
* 대규모언어모델
    * LM (언어 모델, Language Model) 이란, 인간의 언어를 이해하고 생성하도록 훈련된 일종의 인공지능 모델
        * 주어진 언어 내에서 패턴이나 구조, 관계를 학습하여 텍스트 번역과 같은 좁은 AI 작업에서 주로 활용
        * 언어 모델의 품질은 크기나 훈련된 데이터의 양 및 다양성, 훈련 중에 사용된 학습 알고리즘의 복잡성에 따라 달라짐
    * LLM (Large Language Model) 이란, 대용량의 언어 모델을 의미
    *  딥 러닝 알고리즘과 통계 모델링을 통해 자연어 처리(Natural Language Processing, NLP) 작업을 수행하는 데에 사용
    * 사전에 대규모의 언어 데이터를 학습하여 문장 구조나 문법, 의미 등을 이해하고 생성할 수 있음
* 대규모멀티모달모델
    * LMM (Large Multimodal Models)
    * 이미지나 사진 등으로 AI에게 명령을 하면 AI가 이미지를 이해하고 분석하여 답변할 수 있음
    * 텍스트 이상으로 이미지, 사진만을 학습뿐만 아니라 영상, 생체인식, 사람의 제스처 등 지속적으로 학습 요소들이 추가되어 이 이상을 AI가 이해하고 분석하는 기술

---

### 본문의 근거 

* 생성형 AI 챗GPT 열풍으로 대규모언어모델(LLM) 개발 경쟁이 본격화한 지 1년 만에 기술 트렌드에 변화가 생긴 것이다
* GPT-4V는 20개 언어를 감지할 수 있다고 보고서는 소개했다

---

### 추가 조사할 내용 / 결과 
#### 기사의 근거를 통해 바뀐 수치는 무엇인가?

* GPT-4V
    * 시각적 입력을 분석하고 이를 사용하여 출력을 생성하기 위해 OpenAI가 설계한 멀티모달 모델
    * GPT-4V를 통해 사용자는 시각적 입력을 입력하고 이러한 입력에 대한 질문에 대한 답변을 생성할 수 있음
    * 다국어 지원
        * 중국어, 프랑스어, 체코어, 이탈리아어, 한국어, 일본어, 아랍어 등을 포함한 20개 언어
    * 감정 감지
        * 주어진 인물 사진이나 얼굴 입력에서 사람의 얼굴을 분석하고 감정에 대한 판단을 내릴 수 있음
        * 행복, 놀람, 경멸, 슬픔, 두려움, 혐오, 분노 등 7가지 보편적인 얼굴 표정을 이해하는 데 성공
    * 제공 기능
        * 이미지 분석
            * 주어진 비주얼을 분석하고 사용자의 프롬프트에 따라 출력을 생성
            * 수학 문제를 풀거나 책을 번역하거나 다양한 시나리오에 대한 시각적 분석을 수행
        * 이미지 프롬프트 생성/편집
            * 모델에 이미지와 텍스트 요구 사항을 제공하면 원하는 대로 이미지를 편집할 수 있는 프롬프트를 얻을 수 있음
        * 탐색
            * 모델에 방, 거리 또는 고속도로 이미지를 지정하여 내비게이션 출력을 얻을 수 있음
            * 예를 들어, GPT-4V에 방 이미지와 이미지의 특정 지점으로 이동하라는 메시지를 지정하면 경로를 그려서 텍스트 형식으로 출력할 수 있음
        * 비디오 분석
            * 정보를 얻기 위해 몇 시간 동안 동영상을 시청하고 싶지 않다면 GPT-4V 모델을 사용하여 동영상을 분석할 수 있음


--- 
#### 연관기사 링크

[챗GPT 가고 GPT-4V가 온다…‘존맛탱’ 번역하고 엑스레이 영상도 분석 - 사이언스조선](https://biz.chosun.com/science-chosun/technology/2023/10/16/CP2ZP7FY6ZD4DL3NXDBJXEBDTI/)

[챗GPT, 이제는 보고 듣고 말한다...유료 모드에 멀티모달 장착 - AI타임즈](https://www.aitimes.com/news/articleView.html?idxno=153968)

[챗GPT 이미지도 만든다···구글도 ‘멀티모달’ 기능 테스트 - 경향신문](https://m.khan.co.kr/economy/industry-trade/article/202309211710001#c2b)
